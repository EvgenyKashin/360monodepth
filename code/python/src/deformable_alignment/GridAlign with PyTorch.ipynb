{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torch.nn\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use CUDA if available\n",
    "# device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 384\n",
    "disp = torch.tile(torch.arange(imsize, dtype=torch.float, device=device).view(-1, 1), (1, imsize))\n",
    "disp1 = disp + 100 * torch.rand(disp.shape, device=device)\n",
    "disp2 = 500 - disp + 100 * torch.rand(disp.shape, device=device)\n",
    "\n",
    "def norm_disp_midas(d):\n",
    "    \"Normalise disparity maps a la MiDaS (Eq. 5+6).\"\n",
    "    t = torch.median(d)\n",
    "    s = torch.abs(d - t).mean()\n",
    "    print(f\"MiDaS normalisation: t={t:.3f}, s={s:.3f}\")\n",
    "    return (d - t) / s\n",
    "\n",
    "disp1 = norm_disp_midas(disp1)\n",
    "disp2 = norm_disp_midas(disp2)\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"disp\")\n",
    "plt.imshow(disp.cpu())\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"disp1\")\n",
    "plt.imshow(disp1.cpu())\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"disp2\")\n",
    "plt.imshow(disp2.cpu())\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise scale + offset parameters\n",
    "gridsize = 17\n",
    "scale  = torch.ones(1, 1, gridsize, gridsize, requires_grad=True, device=device)\n",
    "# scale  = torch.rand(1, 1, gridsize, gridsize, requires_grad=True, device=device)  # random initialisation\n",
    "offset = torch.zeros(1, 1, gridsize, gridsize, requires_grad=True, device=device)\n",
    "upscale = torch.nn.Upsample(size=disp.shape, mode='bilinear', align_corners=True)\n",
    "\n",
    "# optimizer = torch.optim.SGD([scale, offset], lr=0.001)\n",
    "optimizer = torch.optim.Adam([scale, offset], lr=.01)\n",
    "\n",
    "for iteration in range(1000):\n",
    "    disp2_scaled = upscale(scale) * disp2 + upscale(offset)\n",
    "    data_residual = disp1 - disp2_scaled\n",
    "    data_term = (data_residual ** 2).mean()  # no robust function\n",
    "    \n",
    "    # Hedman & Kopf 2018, Eq. 6\n",
    "    smoothness_term_x = ((scale[:, :, :, 1:] - scale[:, :, :, :-1]) ** 2).mean() + ((offset[:, :, :, 1:] - offset[:, :, :, :-1]) ** 2).mean()\n",
    "    smoothness_term_y = ((scale[:, :, 1:, :] - scale[:, :, :-1, :]) ** 2).mean() + ((offset[:, :, 1:, :] - offset[:, :, :-1, :]) ** 2).mean()\n",
    "    smoothness_term = 1e3 * (smoothness_term_x + smoothness_term_y)\n",
    "    \n",
    "    scale_term = torch.zeros(1, device=device).squeeze()\n",
    "#     scale_term = 1e-4 * (1. / (scale + 1e-6)).mean()  # Hedman & Kopf 2018, Eq. 7\n",
    "#     scale_term = 1e-4 * ((1 - scale) ** 2).mean()  # suggested squared error term\n",
    "    \n",
    "    loss = data_term + smoothness_term + scale_term\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        print(f\"{iteration:3d}. loss = {loss.detach().cpu().numpy():.3f} = {data_term.detach().cpu().numpy():.3f} + {smoothness_term.detach().cpu().numpy():.3f} + {scale_term.detach().cpu().numpy():.3f}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise scale + offset\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"scale\")\n",
    "plt.imshow(scale[0,0].detach().cpu(), vmin=-2, vmax=2)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"offset\")\n",
    "plt.imshow(offset[0,0].detach().cpu(), vmin=-2, vmax=2)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "plt.title(\"GT (disp1)\")\n",
    "plt.imshow(disp1.cpu())\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"disp2_scaled\")\n",
    "plt.imshow(disp2_scaled[0,0].detach().cpu())\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"data_residual\")\n",
    "plt.imshow(data_residual[0,0].detach().cpu())\n",
    "plt.colorbar();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
